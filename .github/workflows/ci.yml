name: CI Pipeline - Exercice 2

on:
  push:
    branches: [ main, dev-quality-system ]
  pull_request:
    branches: [ main, dev-quality-system ]

jobs:
  run-all-tests:
    runs-on: ubuntu-latest
    name: ğŸ§ª ExÃ©cuter tous les tests
    timeout-minutes: 15
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
    
    - name: ğŸ Setup Python 3.13
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
        cache: 'pip'
    
    - name: ğŸ“¦ Install dependencies
      run: |
        pip install pipenv
        pipenv install --dev
    
    - name: ğŸ“ Run linter (flake8)
      run: |
        echo "=== LINTER ==="
        # Utilisation de --exit-zero pour continuer mÃªme en cas d'erreurs de lint
        pipenv run flake8 . --count --max-complexity=15 --max-line-length=127 --exit-zero
        echo "âœ… Linter exÃ©cutÃ©"
    
    - name: ğŸ—„ï¸ Setup Django database (SQLite)
      run: |
        echo "=== CONFIGURATION BASE DE DONNÃ‰ES ==="
        pipenv run python manage.py migrate --noinput
        echo "âœ… Base de donnÃ©es SQLite configurÃ©e"
    
    - name: ğŸ§ª Run Django tests and generate JSON report
      id: django-tests
      run: |
        echo "=== TESTS DJANGO ==="
        # ExÃ©cute les tests. Le '|| true' permet de ne pas faire Ã©chouer l'Ã©tape si les tests Ã©chouent
        pipenv run python manage.py test --noinput --verbosity=2 || true
        
        echo "=== GÃ‰NÃ‰RATION RAPPORT TESTS DJANGO ==="
        pipenv run python generate_test_report.py
        
        # DÃ©but du bloc de correction pour le Bash if/else
        if [ -f "result_test_auto.json" ]; then
          echo "âœ… Rapport Django gÃ©nÃ©rÃ©: result_test_auto.json"
          
          # Script Python inline pour le rÃ©sumÃ© des rÃ©sultats
          pipenv run python << 'PYTHON_EOF'
import json

with open('result_test_auto.json') as f:
    data = json.load(f)

if 'statistics' in data:
    s = data['statistics']
    print('ğŸ“Š RÃ‰SUMÃ‰ TESTS DJANGO:')
    print(f'  âœ… PassÃ©s: {s["passed"]}')
    print(f'  âŒ Ã‰chouÃ©s: {s["failed"]}')
    print(f'  ğŸ‘¤ Manuels: {s["manual"]}')
    print(f'  ğŸ“‹ Total: {s["total"]}')
    
    if s.get('manual', 0) > 0 and 'tests' in data:
        print('\nğŸ” TESTS MANUELS RESTANTS:')
        for test_id, details in data['tests'].items():
            if isinstance(details, dict) and details.get('status') == 'manual':
                print(f'  - {test_id}: {details.get("note", "")}')
PYTHON_EOF
          
        else
          echo "âŒ Rapport Django non gÃ©nÃ©rÃ©. Fin du pipeline."
          exit 1
        fi
    
    - name: ğŸŒ Start Django server for Selenium
      run: |
        echo "=== DÃ‰MARRAGE SERVEUR DJANGO ==="
        pipenv run python manage.py runserver 0.0.0.0:8000 &
        echo $! > django_server.pid
        sleep 5
        
        if curl -f http://localhost:8000/ > /dev/null 2>&1; then
          echo "âœ… Serveur Django dÃ©marrÃ©"
        else
          echo "âŒ Serveur Django non accessible"
          exit 1
        fi
    
    - name: ğŸŒ Run Selenium tests and generate JSON report
      id: selenium-tests
      run: |
        echo "=== TESTS SELENIUM ==="
        
        # Installation de Chrome et ChromeDriver
        sudo apt-get update
        sudo apt-get install -y wget unzip
        
        wget -q -O chrome.deb "https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb"
        sudo apt-get install -y ./chrome.deb || sudo apt-get install -f -y
        rm -f chrome.deb
        
        CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d'.' -f1)
        wget -q "https://storage.googleapis.com/chrome-for-testing-public/${CHROME_VERSION}.0.6099.109/linux64/chromedriver-linux64.zip"
        unzip -o chromedriver-linux64.zip
        sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver
        rm -rf chromedriver-linux64.zip chromedriver-linux64
        
        echo "ğŸ§ª ExÃ©cution des tests Selenium..."
        sleep 3
        
        if pipenv run python selenium_test.py; then
          echo "âœ… Tests Selenium exÃ©cutÃ©s"
        else
          echo "âš ï¸ Tests Selenium avec erreurs"
        fi
        
        if [ -f "result_test_selenium.json" ]; then
          echo "âœ… Rapport Selenium gÃ©nÃ©rÃ©: result_test_selenium.json"
          
          # Script Python inline pour le rÃ©sumÃ© des rÃ©sultats Selenium
          pipenv run python << 'PYTHON_EOF'
import json

try:
    with open('result_test_selenium.json') as f:
        data = json.load(f)
    
    summary = data.get('summary', {})
    passed = summary.get('passed', 0)
    failed = summary.get('failed', 0)
    total = summary.get('total', 0)
    
    print('ğŸ“Š RÃ‰SUMÃ‰ TESTS SELENIUM:')
    print(f'  âœ… PassÃ©s: {passed}')
    print(f'  âŒ Ã‰chouÃ©s: {failed}')
    print(f'  ğŸ“‹ Total: {total}')
    
except Exception as e:
    print(f'âš ï¸ Erreur: {e}')
PYTHON_EOF
          
        else
          echo "âš ï¸ Aucun rapport Selenium"
        fi
    
    - name: â™¿ Run accessibility tests and generate JSON report
      id: accessibility-tests
      run: |
        echo "=== TESTS D'ACCESSIBILITÃ‰ ==="
        
        # Installation de Node.js et Pa11y-CI
        curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
        sudo apt-get install -y nodejs
        
        sudo npm install -g pa11y-ci@5
        
        if ! curl -f http://localhost:8000/ > /dev/null 2>&1; then
          echo "âŒ Serveur non accessible"
          exit 1
        fi
        
        # ExÃ©cution de Pa11y-CI. Le '|| true' Ã©vite l'Ã©chec si des erreurs d'accessibilitÃ© sont trouvÃ©es.
        pa11y-ci --json http://localhost:8000/ > accessibility_report.json 2>/dev/null || true
        
        if [ -f "accessibility_report.json" ]; then
          echo "âœ… Rapport accessibilitÃ© gÃ©nÃ©rÃ©: accessibility_report.json"
          
          # Script Python inline pour le rÃ©sumÃ© des rÃ©sultats d'accessibilitÃ©
          pipenv run python << 'PYTHON_EOF'
import json

try:
    with open('accessibility_report.json') as f:
        data = json.load(f)
    
    if isinstance(data, list) and len(data) > 0:
        result = data[0]
        issues = result.get('issues', [])
        
        errors = [i for i in issues if i.get('type') == 'error']
        warnings = [i for i in issues if i.get('type') == 'warning']
        
        print('ğŸ“Š RÃ‰SUMÃ‰ TESTS ACCESSIBILITÃ‰:')
        print(f'  âŒ Erreurs: {len(errors)}')
        print(f'  âš ï¸ Warnings: {len(warnings)}')
        print(f'  ğŸ“‹ Total: {len(issues)}')
    
except Exception as e:
    print(f'âš ï¸ Erreur: {e}')
PYTHON_EOF
          
        else
          echo "âš ï¸ Aucun rapport accessibilitÃ©"
        fi
    
    - name: ğŸ›‘ Stop Django server
      if: always()
      run: |
        if [ -f "django_server.pid" ]; then
          kill $(cat django_server.pid) 2>/dev/null || true
          rm -f django_server.pid
        fi
    
    - name: ğŸ“Š Run comprehensive test report script
      id: test-report
      run: |
        echo "=== RAPPORT GLOBAL ==="
        pipenv run python test_report.py || echo "âš ï¸ Rapport global avec erreurs"
    
    - name: ğŸ“ Upload test reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-reports
        path: |
          result_test_auto.json
          result_test_selenium.json
          accessibility_report.json
        retention-days: 30
    
    - name: ğŸ’¬ Comment on Pull Request
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        script: |
          const fs = require('fs');
          
          try {
            let djangoData = { statistics: { passed: 0, failed: 0, manual: 0, total: 0 } };
            let seleniumData = { summary: { passed: 0, failed: 0, total: 0 } };
            let accessibilityData = { issues: [] };
            
            try { djangoData = JSON.parse(fs.readFileSync('result_test_auto.json', 'utf8')); } catch (e) { console.log('Django report not found or invalid.'); }
            try { seleniumData = JSON.parse(fs.readFileSync('result_test_selenium.json', 'utf8')); } catch (e) { console.log('Selenium report not found or invalid.'); }
            try {
              const acc = JSON.parse(fs.readFileSync('accessibility_report.json', 'utf8'));
              if (Array.isArray(acc) && acc.length > 0) accessibilityData = acc[0];
            } catch (e) { console.log('Accessibility report not found or invalid.'); }
            
            const django = djangoData.statistics;
            const selenium = seleniumData.summary;
            const issues = accessibilityData.issues || [];
            
            const errors = issues.filter(i => i.type === 'error').length;
            const warnings = issues.filter(i => i.type === 'warning').length;
            
            // Total des tests automatisÃ©s (Django total - Django manuel) + Selenium total
            const totalAuto = (django.total - django.manual) + selenium.total;
            const totalPassed = django.passed + selenium.passed;
            const successRate = totalAuto > 0 ? (totalPassed / totalAuto * 100).toFixed(1) : '0';
            
            const hasFailed = django.failed > 0 || selenium.failed > 0 || errors > 0;
            const status = hasFailed ? 'âŒ Ã‰CHEC GLOBAL' : 'âœ… SUCCÃˆS';
            
            let comment = `## ${status} RÃ©sultats des Tests - PR #${context.issue.number}\n\n`;
            comment += `### ğŸ§ª Tests Django\n`;
            comment += `âœ”ï¸ PassÃ©s: ${django.passed}\nâŒ Ã‰chouÃ©s: ${django.failed}\n`;
            comment += `ğŸ‘¤ Manuels: ${django.manual}\nğŸ“‹ Total: ${django.total}\n\n`;
            
            comment += `### ğŸŒ Tests Selenium\n`;
            comment += `âœ”ï¸ PassÃ©s: ${selenium.passed}\nâŒ Ã‰chouÃ©s: ${selenium.failed}\nğŸ“‹ Total: ${selenium.total}\n\n`;
            
            comment += `### â™¿ AccessibilitÃ©\n`;
            comment += `âŒ Erreurs: ${errors}\nâš ï¸ Warnings: ${warnings}\nğŸ“‹ ProblÃ¨mes: ${issues.length}\n\n`;
            
            comment += `### ğŸ“ˆ RÃ©sumÃ© Global\n`;
            comment += `${status} Tests automatisÃ©s passÃ©s: ${totalPassed}/${totalAuto}\n`;
            comment += `ğŸ“Š Taux de rÃ©ussite: ${successRate}%\n\n`;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
            
          } catch (error) {
            console.error('Erreur lors de la crÃ©ation du commentaire de PR:', error);
          }
