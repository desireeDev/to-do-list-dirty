name: CI Pipeline - Exercice 2

on:
  push:
    branches: [ main, dev-quality-system ]
  pull_request:
    branches: [ main, dev-quality-system ]  # â† AJOUTÃ‰ ICI

jobs:
  run-all-tests:
    runs-on: ubuntu-latest
    name: ğŸ§ª ExÃ©cuter tous les tests
    timeout-minutes: 15
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
    
    - name: ğŸ Setup Python 3.13
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'
        cache: 'pip'
    
    - name: ğŸ“¦ Install dependencies
      run: |
        pip install pipenv
        pipenv install --dev
    
    - name: ğŸ“ Run linter (flake8)
      run: |
        echo "=== LINTER ==="
        pipenv run flake8 . --count --max-complexity=15 --max-line-length=127 --exit-zero
        echo "âœ… Linter exÃ©cutÃ©"
    
    - name: ğŸ—„ï¸ Setup Django database (SQLite)
      run: |
        echo "=== CONFIGURATION BASE DE DONNÃ‰ES ==="
        pipenv run python manage.py migrate --noinput
        echo "âœ… Base de donnÃ©es SQLite configurÃ©e"
    
    - name: ğŸ§ª Run Django tests and generate JSON report
      id: django-tests
      run: |
        echo "=== TESTS DJANGO ==="
        pipenv run python manage.py test --noinput --verbosity=2 || echo "âš ï¸ Certains tests Django ont Ã©chouÃ©"
        
        echo "=== GÃ‰NÃ‰RATION RAPPORT TESTS DJANGO ==="
        pipenv run python generate_test_report.py
        
        if [ -f "result_test_auto.json" ]; then
          echo "âœ… Rapport Django gÃ©nÃ©rÃ©: result_test_auto.json"
          
           pipenv run python << 'EOF
import json

with open('result_test_auto.json') as f:
    data = json.load(f)

if 'statistics' in data:
    s = data['statistics']
    print('ğŸ“Š RÃ‰SUMÃ‰ TESTS DJANGO:')
    print(f'  âœ… PassÃ©s: {s[\"passed\"]}')
    print(f'  âŒ Ã‰chouÃ©s: {s[\"failed\"]}')
    print(f'  ğŸ‘¤ Manuels: {s[\"manual\"]}')
    print(f'  ğŸ“‹ Total: {s[\"total\"]}')
    
    if s.get('manual', 0) > 0 and 'tests' in data:
        print('\\nğŸ” TESTS MANUELS RESTANTS:')
        for test_id, details in data['tests'].items():
            if isinstance(details, dict) and details.get('status') == 'manual':
                print(f'  - {test_id}: {details.get(\"note\", \"\")}')
        "
        else
          echo "âŒ Rapport Django non gÃ©nÃ©rÃ©"
          exit 1
        fi
    
    - name: ğŸŒ Start Django server for Selenium
      run: |
        echo "=== DÃ‰MARRAGE SERVEUR DJANGO ==="
        pipenv run python manage.py runserver 0.0.0.0:8000 &
        echo $! > django_server.pid
        sleep 5
        
        if curl -f http://localhost:8000/ > /dev/null 2>&1; then
          echo "âœ… Serveur Django dÃ©marrÃ©"
        else
          echo "âŒ Serveur Django non accessible"
          exit 1
        fi
    
    - name: ğŸŒ Run Selenium tests and generate JSON report
      id: selenium-tests
      run: |
        echo "=== TESTS SELENIUM ==="
        
        echo "ğŸ“¦ Installation de Chrome..."
        sudo apt-get update
        sudo apt-get install -y wget unzip
        
        # Chrome
        wget -q -O chrome.deb "https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb"
        sudo apt-get install -y ./chrome.deb || sudo apt-get install -f -y
        rm -f chrome.deb
        
        # ChromeDriver
        CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d'.' -f1)
        wget -q "https://storage.googleapis.com/chrome-for-testing-public/${CHROME_VERSION}.0.6099.109/linux64/chromedriver-linux64.zip"
        unzip -o chromedriver-linux64.zip
        sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver
        rm -rf chromedriver-linux64.zip chromedriver-linux64
        
        echo "âœ… Chrome et ChromeDriver installÃ©s"
        
        echo "ğŸ§ª ExÃ©cution des tests Selenium..."
        sleep 3
        
        if pipenv run python selenium_test.py; then
          echo "âœ… Tests Selenium exÃ©cutÃ©s"
        else
          echo "âš ï¸ Tests Selenium avec erreurs"
        fi
        
        if [ -f "result_test_selenium.json" ]; then
          echo "âœ… Rapport Selenium gÃ©nÃ©rÃ©: result_test_selenium.json"
          
          pipenv run python -c "
import json

try:
    with open('result_test_selenium.json') as f:
        data = json.load(f)
    
    summary = data.get('summary', {})
    passed = summary.get('passed', 0)
    failed = summary.get('failed', 0)
    total = summary.get('total', 0)
    
    print('ğŸ“Š RÃ‰SUMÃ‰ TESTS SELENIUM:')
    print(f'  âœ… PassÃ©s: {passed}')
    print(f'  âŒ Ã‰chouÃ©s: {failed}')
    print(f'  ğŸ“‹ Total: {total}')
    
except Exception as e:
    print(f'âš ï¸ Erreur: {e}')
            "
        else
          echo "âš ï¸ Aucun rapport Selenium"
        fi
    
    - name: â™¿ Run accessibility tests and generate JSON report
      id: accessibility-tests
      run: |
        echo "=== TESTS D'ACCESSIBILITÃ‰ ==="
        
        echo "ğŸ“¦ Installation de Node.js..."
        curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
        sudo apt-get install -y nodejs
        
        echo "ğŸ“¦ Installation de pa11y-ci..."
        sudo npm install -g pa11y-ci@5
        
        echo "âœ… pa11y-ci installÃ©"
        
        if ! curl -f http://localhost:8000/ > /dev/null 2>&1; then
          echo "âŒ Serveur non accessible"
          exit 1
        fi
        
        echo "ğŸ§ª ExÃ©cution des tests d'accessibilitÃ©..."
        
        # Tester uniquement la page d'accueil
        pa11y-ci --json http://localhost:8000/ > accessibility_report.json 2>/dev/null || true
        
        if [ -f "accessibility_report.json" ]; then
          echo "âœ… Rapport accessibilitÃ© gÃ©nÃ©rÃ©: accessibility_report.json"
          
          pipenv run python -c "
import json

try:
    with open('accessibility_report.json') as f:
        data = json.load(f)
    
    if isinstance(data, list) and len(data) > 0:
        result = data[0]
        issues = result.get('issues', [])
        
        errors = [i for i in issues if i.get('type') == 'error']
        warnings = [i for i in issues if i.get('type') == 'warning']
        
        print('ğŸ“Š RÃ‰SUMÃ‰ TESTS ACCESSIBILITÃ‰:')
        print(f'  âŒ Erreurs: {len(errors)}')
        print(f'  âš ï¸  Warnings: {len(warnings)}')
        print(f'  ğŸ“‹ Total: {len(issues)}')
    
except Exception as e:
    print(f'âš ï¸ Erreur: {e}')
            "
        else
          echo "âš ï¸ Aucun rapport accessibilitÃ©"
        fi
    
    - name: ğŸ›‘ Stop Django server
      if: always()
      run: |
        echo "=== ARRÃŠT SERVEUR ==="
        if [ -f "django_server.pid" ]; then
          kill $(cat django_server.pid) 2>/dev/null || true
          rm -f django_server.pid
          echo "âœ… Serveur arrÃªtÃ©"
        fi
    
    - name: ğŸ“Š Run comprehensive test report script
      id: test-report
      run: |
        echo "=== RAPPORT GLOBAL ==="
        echo "ğŸ“‹ ExÃ©cution de test_report.py..."
        
        if pipenv run python test_report.py; then
          echo "âœ… Rapport global gÃ©nÃ©rÃ©"
        else
          echo "âš ï¸ Rapport global avec erreurs"
        fi
    
    - name: ğŸ“ Upload test reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-reports
        path: |
          result_test_auto.json
          result_test_selenium.json
          accessibility_report.json
        retention-days: 30
    
    - name: ğŸ’¬ Comment on Pull Request
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        script: |
          const fs = require('fs');
          
          try {
            // Lire les rapports
            let djangoData = { statistics: { passed: 0, failed: 0, manual: 0, total: 0 } };
            let seleniumData = { summary: { passed: 0, failed: 0, total: 0 } };
            let accessibilityData = { issues: [] };
            
            try {
              djangoData = JSON.parse(fs.readFileSync('result_test_auto.json', 'utf8'));
            } catch (e) {
              console.log('âš ï¸ Rapport Django non lisible:', e.message);
            }
            
            try {
              seleniumData = JSON.parse(fs.readFileSync('result_test_selenium.json', 'utf8'));
            } catch (e) {
              console.log('âš ï¸ Rapport Selenium non lisible:', e.message);
            }
            
            try {
              const accData = JSON.parse(fs.readFileSync('accessibility_report.json', 'utf8'));
              if (Array.isArray(accData) && accData.length > 0) {
                accessibilityData = accData[0];
              }
            } catch (e) {
              console.log('âš ï¸ Rapport accessibilitÃ© non lisible:', e.message);
            }
            
            const django = djangoData.statistics;
            const selenium = seleniumData.summary;
            const issues = accessibilityData.issues || [];
            
            const errors = issues.filter(i => i.type === 'error').length;
            const warnings = issues.filter(i => i.type === 'warning').length;
            
            // Calcul totaux
            const totalAuto = (django.total - django.manual) + selenium.total;
            const totalPassed = django.passed + selenium.passed;
            const successRate = totalAuto > 0 ? (totalPassed / totalAuto * 100).toFixed(1) : '0';
            
            // DÃ©terminer l'Ã©tat global
            const hasFailedTests = django.failed > 0 || selenium.failed > 0;
            const hasAccessibilityErrors = errors > 0;
            const overallStatus = hasFailedTests || hasAccessibilityErrors ? 'âŒ' : 'âœ…';
            
            // CrÃ©er le commentaire
            let comment = `## ${overallStatus} RÃ©sultats des Tests - PR #${context.issue.number}\n\n`;
            comment += `**ExÃ©cutÃ© le:** ${new Date().toLocaleString('fr-FR')}\n`;
            comment += `**Branche source:** ${context.payload.pull_request.head.ref}\n`;
            comment += `**Branche cible:** ${context.payload.pull_request.base.ref}\n\n`;
            
            comment += `### ğŸ§ª Tests Django\n`;
            comment += `âœ… **PassÃ©s:** ${django.passed}\n`;
            comment += `âŒ **Ã‰chouÃ©s:** ${django.failed}\n`;
            if (django.manual > 0) comment += `ğŸ‘¤ **Manuels:** ${django.manual}\n`;
            comment += `ğŸ“‹ **Total:** ${django.total}\n\n`;
            
            comment += `### ğŸŒ Tests Selenium\n`;
            comment += `âœ… **PassÃ©s:** ${selenium.passed}\n`;
            comment += `âŒ **Ã‰chouÃ©s:** ${selenium.failed}\n`;
            comment += `ğŸ“‹ **Total:** ${selenium.total}\n\n`;
            
            comment += `### â™¿ Tests AccessibilitÃ©\n`;
            comment += `âŒ **Erreurs:** ${errors}\n`;
            comment += `âš ï¸  **Warnings:** ${warnings}\n`;
            comment += `ğŸ“‹ **ProblÃ¨mes:** ${issues.length}\n\n`;
            
            comment += `### ğŸ“ˆ RÃ©sumÃ© Global\n`;
            comment += `${overallStatus} **Tests automatisÃ©s passÃ©s:** ${totalPassed}/${totalAuto}\n`;
            comment += `ğŸ“Š **Taux de rÃ©ussite:** ${successRate}%\n\n`;
            
            // Tests manuels
            if (django.manual > 0) {
              comment += `### âš ï¸ Tests Manuels Restants\n`;
              comment += `Il reste **${django.manual}** test(s) manuel(s) Ã  rÃ©aliser:\n\n`;
              
              if (djangoData.tests) {
                for (const [testId, testData] of Object.entries(djangoData.tests)) {
                  if (testData.status === 'manual') {
                    comment += `- **${testId}:** ${testData.note || 'Test manuel requis'}\n`;
                  }
                }
              } else {
                comment += `- TC022: Test manuel requis\n`;
                comment += `- TC023: Test visuel requis\n`;
              }
              comment += `\n`;
            }
            
            // Avertissements si Ã©checs
            if (django.failed > 0) {
              comment += `### ğŸš¨ Tests Django Ã‰chouÃ©s\n`;
              comment += `**${django.failed}** test(s) Django ont Ã©chouÃ©. VÃ©rifiez les logs pour plus de dÃ©tails.\n\n`;
            }
            
            if (selenium.failed > 0) {
              comment += `### ğŸš¨ Tests Selenium Ã‰chouÃ©s\n`;
              comment += `**${selenium.failed}** test(s) Selenium ont Ã©chouÃ©.\n\n`;
            }
            
            if (errors > 0) {
              comment += `### ğŸš¨ Erreurs d'AccessibilitÃ©\n`;
              comment += `**${errors}** erreur(s) d'accessibilitÃ© dÃ©tectÃ©e(s).\n\n`;
            }
            
            comment += `---\n`;
            comment += `ğŸ“ **Rapports tÃ©lÃ©chargeables:** [TÃ©lÃ©charger les artefacts](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})\n`;
            comment += `*GÃ©nÃ©rÃ© automatiquement par GitHub Actions*\n`;
            
            // Poster le commentaire
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
            
            console.log('âœ… Commentaire ajoutÃ© Ã  la PR');
            
          } catch (error) {
            console.error('âŒ Erreur lors de la crÃ©ation du commentaire:', error);
            
            // Message minimal
            const fallbackComment = `## ğŸ§ª Tests ExÃ©cutÃ©s\n\nLes tests ont Ã©tÃ© exÃ©cutÃ©s pour la PR #${context.issue.number}.\n\n*Rapport dÃ©taillÃ© non disponible - voir les logs pour plus d'informations*`;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: fallbackComment
            });
          }
